\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage[pdf]{pstricks}
%\usepackage{mathtools}
\usepackage{amssymb,amsmath}
\usepackage{textcomp}
\DeclareMathOperator*{\argmin}{arg\,min}

%\usepackage{subcaption}
%\usepackage{float}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage[font=scriptsize]{caption}
%\usepackage{fullpage}
%\setcounter{secnumdepth}{1}
\begin{document}

\title{Fused regression for multi-source gene regulatory network inference}
\author{Kari Y. Lam \and Zachary M. Westrick \and Christian L. M\"{u}ller \and Lionel Christiaen \and Richard Bonneau}
\maketitle

\begin{abstract}
Understanding gene regulatory networks is critical to learning how cells regulate their behavior and respond to external stimuli. To this end, methods for global network inference have been developed and applied to a variety of species. However, most approaches consider the problem of network inference independently in each species, despite evidence that gene regulation can be conserved even in distantly related species. We introduce a method for multi-source network inference that allows simultaneous estimation of gene regulatory networks in multiple species or biological processes through the introduction of priors based on known gene relationships such as orthology. These priors are incorporated using fused regression. This approach improves network inference performance even when orthology mapping and conservation are incomplete. We further refine this method with an algorithm that attempts to learn the true conserved subnetwork from a larger set of potentially conserved interactions. We demonstrate the utility of our method in cross species network inference and in merging data collected on different experimental platform and in incorporating binding similarity predictions.
\end{abstract}

\section{Introduction}
As the volume and variety of genome scale data continues to increase in quantity and quality, the goal of accurately modeling gene regulatory networks has become attainable \cite{bonneau_predictive_2007, ciofani_validated_2012, carro_transcriptional_2010}. Large-scale data collection efforts have contributed to the development of high quality networks which accurately recapitulate biological processes, but most processes and organisms remain uncharacterized at the network level. Furthermore, as new technologies are developed and some old ones are replaced, such as RNAseq and microarray, it becomes important to be able to combine data from multiple platforms, lest we lose valuable information from existing studies. The problem of inferring related -- but not necessarily identical -- structure from related -- but not identical -- data is ubiquitous in biology. Multi-source network inference has applications for learning multiple networks in related species, for learning networks associated with distinct processes within the same species, and for learning networks based on heterogenous data sources. Moreover, as it becomes possible to learn genome-wide regulatory networks, we can begin to compare and to test whether there is conservation of networks across species and biological processes. Our use of model organisms to study biological processes and diseases relevant to humans relies on the assumption of conservation; yet this has not been effectively tested at the genome scale. 

We present two methods for network inference based on linear estimates of gene expression dynamics, extending existing dynamical-systems methods for network inference \cite{bonneau_predictive_2007, arrieta-ortiz_experimentally_2015}. The core of both methods is the observation that biological information about the relatedness of genes can be used to select which network coefficients should be similar to one another in a multi-source network inference problem (ie orthologous TFs should regulate orthologous genes), and that these constraints can be efficiently represented as penalties in a least-squares regression problem. Taking into account the similarity of putatively conserved interactions improves our ability to accurately describe TF-gene relationships on a genome-wide scale.  

Our first method -- fused ridge -- uses an L2 penalty on the differences between \textit{a priori} similar interactions (termed fusion penalty), and is useful where the relationships between networks (similarity of genes between data sets) is reliable. In the case where both networks contain an identical set of genes and TFs, this approach can be thought of as parametrically interpolating between treating the data sources separately and combining them together. In the case of multi-species, simply combining two datasets is both unwise -- because the networks may differ substantially -- but also potentially impossible, because the set of common of genes may be small. Our method allows useful pooling of data even when the overlap between genes is incomplete, or when orthology assignments depart from a strict one-to-one mapping. Our second method -- adaptive fusion -- uses a non-concave saturating fusion penalty to simultaneously infer the constrained networks and to learn which constraints should be relaxed (ie which parts of the network are genuinely different). With this approach, we seek to identify both conserved and divergent interactions between related networks.

In the case of multiple species, numerous studies showed that functional conservation exists in gene regulatory networks even across large evolutionary distance \cite{satou2006gene, hinman2009evolution,tanay2005conservation,erwin2009evolution}. In our fused L2 approach, we assume that for closely related species, orthologous TFs could exert similar regulatory effects on orthologous target genes. These orthology relationships form the basis of a set of constraints which favor -- but do not require -- networks in which orthologous transcription factors regulate orthologous genes. As a result, data in one species can improve network inference performance in another species  (and vice versa). This general framework for multi-species network inference can be extended to an arbitrary number of distinct organisms, each contributing data with only partially overlapping sets of genes. This is an advantage over existing approaches to multi-species network inference, which infer only a sub-network for which orthologs exist in every species \cite{joshi_multi-species_2015}. 

This approach of introducing constraints on the similarity between specific regulatory interactions can be extended beyond the case of multi-species network-inference from orthology; any biological prior on similarity can be used in place of orthology. For example, we can use fused regression to combine datasets obtained using different platforms or experimental techniques. Moreover, we can introduce constraints that favor genes in the same operon or having similar binding sites, towards having similar regulators.

Existing multi species approaches often use orthology as a proxy for functional conservation \cite{penfold_inferring_2015, joshi_multi-species_2015, kashima_simultaneous_2009, zhang2010nearly}, or attempt to learn functional similarity via expression data \cite{gholami_cross-species_2010}. Orthology -- the measure of gene similarity we use -- can be approximated using readily identifiable sequence similarity, which is often a useful predictor of functional similarity \cite{wilson_assessing_2000}. In multi-species network inference, our fused L2 approach minimizes a cost function that strives to simultaneously fit expression data in each species and produce networks that are consistent with evolutionary constraints created using orthology. However, some genes may have taken on different functions and therefore may have new regulatory interactions. For example, gene duplications may lead to neofunctionalization \cite{eisen_phylogenomics:_1998} of the duplicated genes. Even in the case of comparing networks from related cell lines from the same species, changes in chromatin configuration may affect our hypotheses about the similarity of interactions between pleiotropic TFs and target genes \cite{li_role_2007}.


Understanding which interactions are functionally dissimilar is of direct biological interest, but existing approaches to network inference are unable to effectively rule out the existence of a conserved subnetworks. Observing a large difference in the weights of regulatory interactions obtained though independent inference of multiple networks is the best evidence against conservation available with existing network inference techniques. This evidence is very weak, as network inference is typically underconstrained \cite{marbach_revealing_2010-1}, there could be a different set of networks for which conservation does hold, and which fit the data almost as well. We propose, with our adaptive fusion, an approach where network inference occurs simultaneously with the evaluation of conservation, to gather stronger evidence of divergence. 

We approach this problem by introducing a saturating penalty function based on statistical efforts to develop unbiased regularization penalties for regression \cite{zhang2010nearly, fan2001variable}. The main difference between the L2 fusion approach and our new adaptive fusion approach occurs when the difference between presumed analogous interactions is large despite the fusion penalty: we assume that large differences represent evidence that TF-to-target-gene interactions have diverged. We represent this, in adaptive fusion, with a relaxation of the fusion penalty. The resulting cost function is non-convex and difficult to optimize \cite{fan2001variable}; however, we can approximate its solution and obtain deeper insight into functional similarity than is available through strict orthology enforcement or the comparison of separately learned networks. 

Although the fusion constraints we employ can be described as arising from orthology - which links genes - it is important to note that the constraints themselves link individual regulatory interactions. This finer level of representational granularity is critical to the functioning of adaptive fusion, and means the method can accommodate any form of prior on expected similarity between regulatory interactions, even priors that cannot be decomposed into gene to gene mappings. We develop two algorithms for solving efficiently multi-output least-squares regression problems with pairwise L2 fusion penalties on entries of the coefficient matrix. We also introduce - in the form of adaptive fusion - the idea of a saturating penalty function on fusion constraints, and estimate the solution to the resulting optimization problem through iterative application of the fused L2 algorithm.

We test the ability of fused L2 and adaptive fusion to improve network recovery on both synthetic data and by comparing related networks in the bacteria species \textit{Bacillus subtilis} and \textit{Bacillus anthracis}. This shows the applicability of our method in combining different datasets and leveraging similarity across organisms as well as within a network in order to improve network inference. We explore the circumstances under which each approach is optimal, and evaluate the robustness of adaptive fusion to incorrect orthology, simulating the biologically relevant cases of neo- and subfunctionalizations. 


\section{Methods}

\subsection{Statistical approach and background}
We consider prediction and coefficient estimation problems with $N$ observations of $M$ dependent variables $y_{1,1}, y_{2,1}, ...y_{N,1}, y_{N,2},..., y_{N, M}$ and features $x_{i,j}$, $i=1,2,...,N, j=1,2,...,p$. We begin with a standard linear regression model:

\begin{equation}
y_{i,k} = \displaystyle\sum_{j}x_{ij}\beta_{j,k} + \epsilon_i
\end{equation}

with errors $\epsilon_i$ having mean 0 and constant variance, and predictors $x_{ij}$ having mean 0 and unit variance. We are interested in the case where $p > N$. Many methods have been proposed to deal with the underconstrained case, and have been applied to genomic data \cite{waldron_optimized_2011, li_network-constrained_2008}. For example, ridge regression penalizes the L2 norm of the coefficients $\beta_{i,j}$ in order to avoid overfitting \cite{hoerl_ridge_1970}, and can be thought of as a mean-zero Gaussian prior on the coefficients. 

More complicated penalties have been developed to represent certain expected or desireable structure in a regression model's coefficients. For example, Land and Friedman \cite{citeulike3780356} proposed a fusion penalty:

\begin{equation}
\text{argmin}_{\beta} \displaystyle\sum_{ij} || y_i - x_{ij}\beta{j} ||^2 + \sum\limits_{j=2}^p ||\beta_j - \beta_{j-1}]|^\alpha
\end{equation}

Which encourages smoothness of the estimated parameter vector. 

%This penalty utilizes correlation information from data by penalizing pairwise differences of coefficients \cite{Daye2009,Price2014}. 
Existing approaches have used similar fusion penalties to combine statistical strength across multiple regression tasks by introducing regularization functions reflecting prior knowledge on output structure representing how outputs are related \cite{kim_tree-guided_2012,Land1997,Chen2010,Petry2011,Hebiri2011}. Price et al. and Bilgrau et al. use a fused ridge estimator for jointly estimating multiple inverse covariance matrices \cite{Price2014a,Bilgrau2015}; we take a similar approach, adding an L2 penalty on the differences between coefficients to the existing ridge penalty in order to incorporate prior knowledge about relationships between input-output pairs:


\begin{equation}
\argmin_{\beta} \displaystyle\sum \vert \vert X\beta - Y \vert \vert ^2 + \lambda_R \vert \vert \beta \vert \vert ^2 +  \lambda_S  \displaystyle \sum_{\beta_{g,k} \approx \beta_{h,l}} || \beta_{g,k} - \beta_{h,l} ||^2
\label{eqn:main}
\end{equation}

where $X$, $Y$, and $\beta$ are matrices, and $\beta_{g,k} \approx \beta_{h,l}$ denotes fusion between arbitrary entries of $\beta$. Note that, like ridge regression, this penalty can be thought of as representing a Gaussian prior on the coefficients $\beta$. In the case where $\beta$ is a column vector, introducing this penalty is equivalent to assuming that $\beta$ is sampled from a multivariate Gaussian with inverse covariance matrix $\Sigma^{-1} = \lambda_R + \displaystyle \sum_{\beta_g \approx \beta_g} \lambda_S (1_{g,g} + 1_{h,h} - 1_{g,h} - 1_{h,g})$, where $I$ denotes the identity matrix and $1_{i,j}$ a matrix of zeros with 1 in its $i, jth$ entry. In the case of a two-coefficient model with fusion between the coefficients, for example, fused L2 is equivalent to assuming a prior with variance $(\lambda_R + \lambda_S)/(\lambda_R^2+2\lambda_R\lambda_S)$ and covariance $\lambda_S/(\lambda_R^2+2\lambda_R\lambda_S)$.


In many cases, however, there is some uncertainty about the relationships that should be enforced. Sohn et al. attempt to simultaneously learn the regression coefficients and the output structure \cite{sohn_joint_2012}. We take a simultaneous approach, by applying a penalty function bounded by a constant to produce unbiased estimators for large coefficients, combined with an L2 penalty, similar to SCAD-L2 \cite{Zeng2012}. 

\begin{equation}
\argmin_{\beta} \displaystyle\sum \vert \vert X\beta - Y \vert \vert ^2 + \lambda_R \vert \vert \beta \vert \vert ^2 + \displaystyle \lambda_S  \displaystyle \sum_{\beta_{g,k} \approx \beta_{h,l}} p_{\lambda, a} (\beta_{g,k} - \beta_{h,l})
\end{equation}

where the penalty $p_{\lambda, a}$ has derivative 

\begin{equation}
p'_{\lambda,a}(\theta) = \left\{
    \begin{array}{lr}
    2\lambda\theta & \text{if } \theta \leq \lambda\\
    \text{max}(\lambda_S(a-\theta),0) & \text{if } \theta > \frac{a}{2}
    \end{array}
    \right.
\end{equation}

 This approach allows us to simultaneously learn the regression coefficients and evaluate the validity of our prior information. 

\subsection{Application}
Although our approach is generalizable to a wide variety of multi-source network inference problems, we begin with the concrete example of network inference in two related species. Our approach to multi-species network inference is based on the hypothesis that gene regulation in related species is governed by similar but not necessarily identical gene regulatory networks, due to conservation of function through evolution. We represent conservation of network function by introducing constraints into the objective function for network inference that penalize differences between the weights of regulatory interactions believed to be conserved. These constraints favor the generation of similar networks for related species, and in the generally under-constrained regime of network inference can improve the accuracy of network recovery. We then go on to introduce a method to test the assumption of conserved network structure, and to relax the associated constraints on pairs of interactions for which data does not support conservation.

\subsection{Gene regulatory network}

We are interested in identifying genes which affect other genes' expression, and understanding how this gives rise to a given biological process. We know that transcription factors (TFs) can bind to DNA and activate or repress expression of other genes. By restricting ourselves only to TFs and gene interactions, we vastly reduce the space of possible models while considering biologically relevant cases. 

We model the transcription rate of each gene as a weighted sum of transcription factor expression, and seek to identify the identities and regulatory weights of these TFs. This formulation matches that of the existing Inferelator algorithm, which models gene expression with linear differential equations \cite{bonneau_inferelator:_2006-1}. Our primary data for learning gene regulatory networks is expression data, consisting of time-series and steady state experiments. We use time-series data to approximate the rate of change of expression, and treat steady-state experiments as having reached equilibrium (constant expression). The rate at which $x_{i}$, the observed mRNA expression of gene $i$, changes, is governed by degradation of existing transcripts with rate $\alpha$ plus a linear combination of transcription factor (TF) expressions. 
\begin{equation}
\frac{\mathrm d}{\mathrm d t} x_i = -\alpha_{i}x_{i} + \sum \beta_{i,j}x_{j}
\end{equation}
where $\beta_{i,j}$ represents the weight of TF $j$ on gene $i$, and $\alpha$ is the decay rate of gene $i$. We fix the decay rate $\alpha$ for all genes, and set it assuming a time-constant of 10 minutes \cite{hambraeus_genome-wide_2003, selinger_global_2003}, as in \cite{greenfield_robust_2013}. Let $x_i(t)$ be the expression of gene $i$ at time $t$. Given time-series data on the expression of gene $i$ at timepoints $t_k$ and $t_{k+1}$, we can approximate the rate of change of $x_i$ as $x_i'(t_k)=\frac{x_i(t_{k+1})-x_i(t_k)}{t_{k+1}-t_k}$. We treat steady-state data as having a derivative of zero. This gives us, for each gene $i$ and time $t_{k}$ an equation

\begin{equation}
\frac{x_i(t_{k+1})-x_i(t_k)}{t_{k+1}-t_k} + \alpha_{i}x_{i}(t_k)= \sum \beta_{i,j}x_{j}(t_k)
\end{equation}
where $j \neq i$ 
for time series and 
\begin{equation}
\alpha_{i}x_{i}(t_k) = \sum \beta_{i,j}x_{j}(t_k)
\end{equation}
for steady state. 


\noindent We can summarize these equations in matrix form as
\begin{equation}
Y = X \beta 
\end{equation}
where $Y$ is the gene expression matrix, $X$ is the TF expression matrix, and $\beta$ is the regulatory weights we are interested in learning.
We are interested in learning $\beta$, the matrix representation of the gene regulatory network, where the weight in a given position represents the regulatory weight of a TF on a gene. Positive weights represent activation, negative weights represent repression, and 0 weights represent the absence of an interaction. The matrix $\beta$ can be solved using linear regression. Because there are typically far fewer conditions than possible regressors (TFs), we introduce a ridge regularization constraint with weight $\lambda_R$ and solve
\begin{equation}
\argmin_\beta\vert \vert X\beta - Y_2 \vert \vert ^2 + \lambda_R \vert \vert \beta \vert \vert ^2
\end{equation}
This is similar to the formulation used in the Inferelator algorithm, which we extend to the case of simultaneously inferring multiple networks.

Transcription factor expression is not always the best predictor of its gene targets' expression. When there exists a set of prior known interactions, we are able to estimate transcription factor activity (TFA) using network component analysis \cite{liao2003network}, as in \cite{arrieta-ortiz_experimentally_2015, fu_reconstructing_2011}, and use TFA as explanatory variables instead of transcription factor expression. 

\subsection{Fused gene regulatory networks}

Information about the conserved structure of gene regulation is introduced through the incorporation of constraints into the above regression formulation. These constraints penalize differences between interaction weights in the networks of multiple species that are expected to be similar based on prior biological knowledge. We can then solve the penalized regression problems simultaneously, in order to obtain a gene regulatory network (GRN) for each species. Consider the case of organisms $A$ and $B$, governed by GRNs $\beta^A$ and $\beta^B$ (the following approach applies equally well to more than two species but for simplicity we continue with the case of two species). If TF $g^A$ in organism $A$ and TF $h^B$ in organism $B$ are orthologs, and gene $k^A$ and $l^B$ are orthologs, then we expect that the $g^A \rightarrow k^A$ interaction weight should be similar to the $h^B \rightarrow l^B$ interaction weight, and we introduce a fusion constraint between these analogous interactions. In terms of the above regression formulation, we expect that $\beta^A_{g,k} \approx \beta^B_{h,l}$, and include a penalty term $\lambda_Sp(\beta^A_{g,k} - \beta^B_{h,l})$ in the quantity being minimized in order to encourage similarity. The function $p(x)$ controls the shape of the relationship between weight dissimilarity and penalty, while scalar $\lambda_S$ controls the overall scaling of the penalization of differences between fused coefficients. $\lambda_S$ controls the tradeoff between fitting the expression data alone and producing a set of networks that conform to evolutionary prior knowledge. This gives us the final equation to be minimized 

\begin{equation}
%\argmin_{(\beta^A, \beta^B)} \displaystyle\sum_{S \in (A, B)} \vert \vert X^S\beta^S - Y^S \vert \vert ^2 + \lambda_R \vert \vert \beta^S \vert \vert ^2 + \displaystyle \sum_{\beta^{S_1}_{g,k} \approx \beta^{S_2}_{h,l}} p_{\lambda_S}(\beta^{S_1}_{g,k} - \beta^{S_2}_{h,l})
\argmin_{\beta^S} \displaystyle\sum_{S \in \{1, 2\}} \vert \vert X^S\beta^S - Y^S \vert \vert ^2 + \lambda_R \vert \vert \beta^S \vert \vert ^2 + \lambda_S \displaystyle \sum_{\substack{(g,h) \in orth,\\
 (h,l) \in orth}}(p(\beta^{S_1}_{g,k} - \beta^{S_2}_{h,l}))
\end{equation}

where the second sum is over pairs of interactions with fusion constraints. In the fused L2 algorithm we develop, the penalty function is equal to the L2 norm of the difference in regulatory weight of fused coefficients. That is, $p(x)=x^2$. The overall objective function for network inference is then:

\begin{equation}
\argmin_{\beta^S} \displaystyle\sum_{S \in \text{species}} \vert \vert X^S\beta^S - Y^S \vert \vert ^2 + \lambda_R \vert \vert \beta^S \vert \vert ^2 + \displaystyle \lambda_S \sum_{\substack{(g,h) \in orth,\\
 (h,l) \in orth}} \vert \vert (\beta^{S_1}_{g,k} - \beta^{S_2}_{h,l}) \vert \vert ^2
\end{equation}

Because every component of the objective function is an L2 norm, the entire problem is convex, and can in fact be solved through linear regression with an augmented design matrix. %The regulators of a pair of genes can be solved independently as long as the pair is not linked by fusion constraints, or a chain of fusion constraints; thus the problem of cross-species GRN inference decomposes into a large number of manageable regression problems as long as the orthology mappings are sparse.

In order to better understand the behavior of this objective function, consider the case where there is a one-to-one orthology between the species being considered (ie different cell-lines of the same organism). The choice of $\lambda_S$ allows one to interpolate between fitting each network independently ($\lambda_S=0$) and pooling data together as if it came from one source ($\lambda_S=\inf$). In addition to allowing interpolation between these extremes, our method allows pooling of data even when there is incomplete orthology. By introducing constraints on the similarity of individual interactions, rather than on the networks as a whole \cite{liu2011temporal}, we can pool some information across species even when a small fraction of genes have orthologs. This is particularly useful when dealing with a large number of species; pairwise orthologies may be nearly complete even when the number of genes present in every organism is small. 


\subsection{Adaptive fusion}
Fusion constraints are intended to penalize dissimilarity between interactions thought to be analogous based on \textit{a priori} knowledge. For example, orthology can be used to predict which interactions will be similar across species. When fusion constraints are L2, interaction weights which differ from each other by a large amount are excessively penalized, which effectively ensures that fused interactions are assigned similar weights. This will be inappropriate for interactions which are identified based on orthology as being analogous, but which are no longer similar due to evolutionary changes. We propose that a saturating penalty -- a penalty that is relaxed once differences in weights grow beyond a certain point -- may be useful for dealing with uncertainty about which interactions are conserved. 



With a saturating fusion penalty, fused interactions which appear to be very different based on expression data are allowed to unfuse from one another. A closely related problem has been studied in the context of LASSO regularization, where it was shown by Fan and Li that using a saturating penalty retains many of LASSO's desireable properties while removing its bias towards 0 \cite{fan2001variable}. They further showed that, although the resulting loss-function is nonconvex, good results can be obtained with a local quadratic approximation of gradient descent. Several saturating penalties, such as SCAD \cite{fan2001variable} and MCP \cite{zhang2010nearly}, have been discussed in the context of sparse regression. We introduce a modified form of MCP to the problem of penalizing differences between fused coefficients. The principle difference between the penalty we adopt and SCAD/MCP is that both of these penalties are L1 like at the origin, producing sparse solutions. Some network inference approaches use L1 penalties to produce sparse networks, on the basis that biological networks are thought to be sparse. However, since we are penalizing differences in interaction weights, rather than the weights themselves, there's no reason to assume that most differences will be exactly zero, and an L2 penalty - equivalent to an assumption that the differences between fused coefficients are Gaussian distributed - may be more appropriate.

We use a penalty on the difference between fused coefficients $\theta$ which is L2 like at the origin, begins to level out at $\theta = \frac{a}{2}$, and saturates at $\theta = a$. Written in terms of its derivative, the penalty $p'_{\lambda, a}$

\begin{equation}
p'_{\lambda,a}(\theta) = \left\{
    \begin{array}{lr}
    2\lambda\theta & \text{if } \theta \leq \lambda\\
    \text{max}(\lambda_S(a-\theta),0) & \text{if } \theta > \frac{a}{2}
    \end{array}
    \right.
\end{equation}

%We also implement MCP:

%\begin{equation}
%p_{\lambda,\gamma}(\theta) = \left\{
%    \begin{array}{lr}
%    \lambda\theta^2-{ \theta^2 \over 2\gamma} & \text{if } \theta \leq \gamma\lambda\\
%    {1 \over 2}\gamma\lambda^2 & \text{if } \theta > \gamma\lambda
%    \end{array}
%    \right. 
%\end{equation}

%With derivative

%\begin{equation}
%p'_{\lambda,\gamma}(\theta) = \left\{
%    \begin{array}{lr}
%    2\lambda\theta - {\theta \over \gamma} & \text{if } \theta \leq \gamma\lambda\\
%    0 & \text{if } \theta > \gamma\lambda
%    \end{array}
%    \right.
%\end{equation}
    
As in \cite{fan2001variable}, we solve using iterative local quadratic approximation. Specifically, $\beta^S(t)$ is the network on iteration $t$. For each fused $B^{S_1}_{g,k} \approx B^{S_2}_{h,l}$ we define:

\begin{equation} 
\theta(0)=0
\end{equation}
\begin{equation}
\theta(t) = \vert B^{S_1}_{g,k} - B^{S_2}_{h,l} \vert
\end{equation}

and introduce a fusion constraint $\lambda = \frac{p'(\theta(t))}{2\theta(t)} $

$\beta^S(t+1)$ is obtained by fitting the ridge-fused model with fusion constraints given by the above $\lambda_S$. This is useful because all our penalties can be treated as L2 and therefore retain the properties of ridge regression, and can be solved using the fused L2 algorithm we develop.

Our adaptive penalty function introduces, in addition to regularization and fusion penalty weights $\lambda_R$ and $\lambda_S$, an unknown parameter $a$. We could employ grid search using cross-validation to search for the best parameters, but for many data sets, this can be computationally expensive. Moreover, we are primarily interested in using this saturating penalty as a way of testing the hypothesis that conservation in GRNs can be predicted based off of known similarities between genes. Therefore, we propose a method for selecting $a$ to be equal to a certain percentile of the distribution of differences in fused interaction weights from the networks fit without fusion. This choice represents the user's hypothesis for the fraction of both networks that have unrelated function, and should therefore be unfused. 







\subsection{Solving fused L2 problems using augmented matrices}
We begin with the problem of constructing a design matrix to solve a fused L2 regression problem with a single response variable, because problems with multiple response variables and with multiple data sources can be converted into this form through vectorization. We then go on to show that, although the vectorized solution involves solving an impractically large system of equations, under typical biological conditions the structure of constraints allow the problem to be broken up into many smaller subproblems. 

We begin with the observation that ridge constraints can be incorporated into a least-squares regression problem by appending a scaled identity matrix to the design matrix, and a corresponding number of zeros to the response vector. Similarly, a fusion constraint $\lambda_S (\beta_{i} - \beta{j})^2$ can be incorporated into a least-squares regression problem by appending a row containing $\sqrt{\lambda_S}$ in the $i$th position, $-\sqrt{\lambda_S}$ in the $j$th position, and $0$s elsewhere to the design matrix, and zero to the response vector. In order to convert an optimization over multiple response variables and multiple sources into an optimization with a single source and response variable, we vectorize as follows: we construct a new design matrix by diagonally concatenating design matrices from relevant regression problems, and create a new response vector by vertically concatenating corresponding response vectors. 

%$$\underset{W}{min} ||AW - y||_2^{~2}$$

%We can then define $W_{i,j} = \beta_{(j-1) \times N + i, (j-1) \times M + j}$ which is the desired form.
This is equivalent to the original problem due to the block structure of matrix multiplication. In an ordinary regression problem each response variable can be solved independently, and vectorization is unnecessary. However, in fused regression, we append additional rows to the design matrix that link entries of the interaction weight matrix associated with different response variables. As a result, these linked response variables must be solved simultaneously through vectorization. Two response variables are linked by a fusion constraint if any of the regulatory weights affecting those genes are linked by a fusion constraint. Two response variables must be solved simultaneously if there is any chain of linked response variables connecting them. However, every other response variable can be solved separately. In biological terms, the regulators of two genes (whether in the same species, or different species) must be solved together if there is a fusion constraint linking those genes' regulators, or if there is a chain of such constraints. This is important because vectorization produces a design matrix with a number of rows and columns equal to the sum of the number of rows and columns of the design matrices associated with each of the response variables (genes) being solved for. If the networks for a large number of genes are solved simultaneously, the system of equations can quickly become intractible to solve. 

In order to avoid this difficulty, we use depth-first search to identify linked columns of each TF expression matrix, then form design and response matrices through vectorization. We can then incorporate fusion constraints as in the case of single-source single response-variable fused regression. In most cases, we have found the direct solution using augmented matrices to be adequate. This is because the structure of orthology tends to be sparse, so only a small number of genes must be solved at once. In the general case, the size of the design matrix is proportional to the number of response variables that must be solved simultaneously. Because the scaling of this algorithm has a complicated dependence on the constraint structure used, a general description of its runtime is difficult. However, in the case of multi-species network inference with one-to-one orthology, the network associated with each pair of orthologous genes requires solving a linear system with approximately twice as many observations and unknowns as the single species case. Linear systems of this size can be solved quickly using standard techniques. When the size of the groups of genes linked by fusion constraints becomes large, however, the augmented design matrix approach becomes impractical.


%where the $j$s refer to transcription factors. In subproblem 1, $j_1$ regulates gene $g$; $j_1$ is orthologous to $j_2$, a transcription factor of subproblem 2, which regulates $h$, a gene which is orthologous to gene $g$. There exists a chain because ($j_1$, $g$) is a TF, gene pair which is fused with ($j_2$, $h$), which is fused to ($j_3$, $k$).
%We use depth-first search to identify linked columns of $Y$, and then solve the fused regression problem directly by vectorizing those columns.
%In order to incorporate the fusion penalty, we append to the resulting design matrix one row \emph{per connected constraint}, which contains $\lambda_S$ in the column associated with $Y_g,i,j,$ and $-\lambda_S$ to the column associated with $Y_h, i',j'$ for constraint $(Y_g,i,j,Y_h,i',j')$. Call this resulting matrix $X_P$.
%The vector $Y$ is augmented by appending an appropriate number of zeros to produce $Y_P$. As a result, each entry in the output of $X_P b$ in the columns associated with each constraint contain $\lambda_S(b_{i,j} - b_{i',j'})$.
%These entries each contribute  $\lambda_S||b_{i,j} - b_{i',j'}||^{2}$ to the squared error $||X_Pb - Y_P||_2^{~2}$

%In the context of regression for GRN inference with multiple cell sources, each gene $j$ from source $1$ has constraints $\forall_{i \in \{TFs\}} (1, i, j, 2, i’, j’)$ where $i',j'$ are the same gene in a different cell line.
%As a result, each gene has only one constraint and the chain has length $L=2$.
%In general, however, solving the fused regression problem directly, even after factoring constraints, involves at least one inversion of an $L\sum_{i=0}^S K_i \times L \sum_{i=0}^S K_i$ matrix, taking $O((L\sum_{i=0}^S K_i)^3)$.
%In general, however, solving the fused regression problem directly, even after factoring constraints, involves at least one inversion of an $p\sum_{i=0}^S K_i \times p \sum_{i=0}^S K_i$ matrix where $p$ is the size of a column subset such that there is a constraint chain $L$ linking an element of each column to an element in every other column in that subset, taking $O((p\sum_{i=0}^S K_i)^3)$.
%Although the sparse structure of $X_P$ makes this somewhat more tractable, it is still impractical for large $L$.

\subsection{Solving fused L2 problems using iterative solver}

When the number of genes in each fusion constraints is relatively small, the augmented design matrix approach we have described has the advantage of being efficiently solvable with a closed form solution. However, in some cases the augmented system of equations may be too large to solve efficiently with standard methods. To address this limitation, we developed an iterative solver that uses coordinate-wise descent to solve for solutions corresponding to a sequence of values of fusion penalty weights; since our fused L2 method uses a convex and differentiable penalty function, this approach converges to a global minimizer. Although less efficient than the augmented design matrix approach we developed, the iterative solver has the advantage of computing a solution path for $\lambda_S$.

On each iteration $t$ the iterative solver computes


\begin{equation}
\text{argmin}_{\beta^{S_i(t)}, \beta^{S_j(t)}} \displaystyle\sum_{S_i, S_j \in \text{species}} ||X^{S_i}\beta^{S_i}(t) - Y^{S_i}||^2 + \lambda_R||\beta^{S_i}(t)||^2 + \lambda_S\displaystyle \sum_{\beta_{g,k}^{S_i} \approx \beta_{h,l}^{S_j}} ||\beta^{S_i}_{g,k}(t) - \beta_{h,l}^{S_j}(t-1)||^2
\end{equation}
Note that this is almost identical to equation 5, but now the network $\beta$ is a function of the iteration number $t$. On each step, we compute $\beta$s that minimize a penalized cost function where the fusion penalties encourage similarity between a parameter and its fused-to parameter from the previous iteration's solution. This process is iterated until the estimated $\beta$s converge. Because each iteration reduces the error between $\beta(t)$ and $\beta(t-1)$, and because $\beta(t) = \beta(t-1)$ is the globally optimal solution, this process must eventually converge to the same network as equation \ref{eqn:main}. Although we have not produced bounds on the convergence rate, which also depends on the structure of constraints, in practice a small number of iterations are necessary.

The ierative solver is used for multi-source regression problems with complex orthology/similarity mappings, and also for solving the regularization path in order to pick regularization and fusion penalty weights. 

\subsection{Fusion and regularization path}
Because optimizing over both parameters, $\lambda_R$ and $\lambda_S$, is computationally prohibitive, we opted to optimize the two parameters separately. Our procedure first optimized $\lambda_R$ with $\lambda_S=0$, then optimized $\lambda_S$ using this value of $\lambda_R$. Although this procedure is sub-optimal, it is guaranteed to achieve the best unfused solution in the case when $\lambda_S$ is constrained at 0. As a result, any performance gains of fused regression are a lower bound on the highest achievable performance gains. 

To optimize $\lambda_R$ we use cyclical coordinate descent algorithms from the 'glmnet' package \cite{friedman_regularization_2010} to compute a ridge regularization path. We use cross validation to select the optimal $\lambda_R$ parameter from this path, selecting the $\lambda_R$ which minimizes the average error of prediction on a leave out set across cross validation folds. Following selection of $\lambda_R$, we search for optimal $\lambda_S$ by computing the solution path from the iterative solver (using the sequence of successive model weights) again using cross validation to select the optimal parameter. Note that both parameters are chosen without reference to the gold standard, which is used in a separate evaluation of network quality. 



%\subsection{Equivalent prior}
%Our L2 fusion penalty is equivalent to assuming a Gaussian prior with variance proportional to $\frac{1}{\lambda_S}$ on differences in parameters with fusion constraints. Combined with the regularization constraints, this forms a multivariate Gaussian prior on weights in both networks. When using our method on systems with incomplete orthology mapping, this results in different prior probability distributions for those parameters with fusion constraints and those without. We adjust for this by solving for a constant to multiply fused interactions by to equalize the volumes of the prior distributions. The intent of this adjustment is to ensure that constrained interactions are not on average more highly penalized, which may tend to drive their weight towards zero, causing them to be excluded from the network. 

%\subsection{Model selection}
%We can define a leave-out set of priors and choose parameters based on cross-validated AUPR on the leave-out set. 


\subsection{Simulated data}
We generate simulated data to evaluate the ability of our fused L2 approach to learn the true network and to show that sharing information between similar but not identical data sources results in more accurate network recovery. Because the purpose of adaptive fusion is to learn the structure of conservation while also learning networks, using simulated data allows us to test the method relative to a known gold standard for network conservation. 


Generation of simulated data begins with the production of random orthology mappings. We produce a one-to-one orthology by pairing random genes until a specified fraction have been assigned orthologs. This process is carried out separately for TFs and non-TF genes, so that TFs and non-TF genes are never assigned to be orthologous. We then produce a pair of random networks ($B^1$ and $B^2$) as follows. For each unfilled entry in $B^1$ or $B^2$, we enumerate the set $C$ consisting of the entry along with every entry in either matrix to which it is fused. With probability equal to the sparsity rate we assign every entry in $C$ to be 0, otherwise we sample a value $v \sim \mathcal{N}(0,1)$ and independently assign each entry in $C$ to $v + \mathcal{N}(0, \sigma_f^2)$. $\sigma_f$ is a parameter that controls the distribution of differences in the values of fused coefficients, so that the nonzero coefficients of $B^1, B^2$ are distributed as $\mathcal{N}(0, 1 + \sigma_f^2)$.

Given a network $B$, we generate $N$ samples of gene expressions at each of two timepoints. The condition by gene expression matrix for timepoint one, $Y_{T1}$, is sampled randomly from a multivariate Gaussian distribution with identity covariance matrix. $X_{T1}$ is the TF expression sub-matrix of $Y_{T1}$, and consists of columns of $Y_{T1}$ that correspond to TFs. Treating the decay rate as 0, the gene expression matrix at timepoint two, $Y_{T2}$ is sampled as $Y_{T2} = Y_{T1} + BX_{T1} + \epsilon$, where $\epsilon$ is a Gaussian noise term. This process is carried out separately for each network. 

Following generation of simulated data, we may introduce error into the orthology mapping. This can take the form of discarding a specified fraction of true orthologies (governed by a false-negative rate), by introducing random false orthologies (governed by a false-positive rate), or by adding Gaussian noise so that fused interactions are not identical (described above). For convenience, the false-positive rate is specified in units of the number of true orthologs, and not the number of possible orthologs. 

For the purposes of evaluating simulated network recovery, we define a gold standard network as the support of the beta matrices. Priors used in network inference are interactions present in the gold standard. The list of priors can be be manipulated to include false positives and false negatives as with ortholog mapping. 

\subsection{Ranking regulatory hypotheses}
In previous work, betas were rescaled as to form a matrix of confidence scores $S$ as follows
\begin{equation}
S_{i,j} = \frac{\sigma^2_{\text{full model for }y_j}}{\sigma^2_{\text{full model for }y_j \text{ without predictor }i}}
\end{equation}
Computing residuals with respect to the data alone would disregard information gained through fusion, because certain interactions may be large due to fusion, rather than their individual explanatory power. Instead, we used an approximation
\begin{equation}
S_{i,j} = \frac{\sigma^2_{\text{full model for }y_j}}{\sigma^2_{\text{full model for }y_j} + \beta_{i,j}^2 \times var(TF_j)}
\end{equation}
When the residual is zero, then removing regressor $i$ with weight $\beta_{i,j}$ from the model of $j$ will increase the residual by $\beta_{i,j}^2$ times the variance of regressor $i$. This scaling is similar to rescaling according to variance explained relative to an augmented design matrix that includes fusion constraints. The intuition for scaling is that a large $\beta_{i,j}$ may be inferred because of overfitting with a TF that varies little across the available data, or because the regulatory weight is strong. Only in the latter case is a large weight suggestive of a true regulatory interaction.

\subsection{\textit{B. subtilis} and \textit{B. anthracis} data and orthology}
We used a dataset collected for PY79, a derivative of strain 168, available on GEO with accession number GSE67023, and a dataset using BSB1, another derivative of strain 168, available at GEO with accession number GSE27219. We used two datasets for \textit{B. anthracis}, transcription profiling during iron starvation (E-MEXP-2272 on ArrayExpress), and time series over the life cycle (E-MEXP-788 on ArayExpress). We ran Inparanoid to obtain orthology mapping for \textit{B. subtilis} and \textit{B. anthracis} \cite{ostlund_inparanoid_2010}

\section{Results}
We used both synthetic networks and real data to test the ability of fused regression to improve the performance of network inference, and the ability of adaptive fusion to identify conserved interactions between orthologous genes. For the synthetic data, we generated random pairs of networks in which orthologous genes have similar regulatory interactions, then sampled gene expression from these networks, which we used to derive learned networks for comparison with the input (true) networks. For real data, we computed recovery of a known gold standard in \textit{Bacillus subtilis}. We measured network inference performance with the area under the precision recall curve (AUPR). 

\subsection{Using fused regression to learn related networks}
It is known that the accuracy of network inference improves with additional data \cite{bar-joseph_computational_2003}. Additional data is not always readily available and does not always make identification of unique network weights possible; but often there is an abundance of related data. Our method allows this data to be used in network inference. Using related data for network inference allows us to borrow statistical power from other sources, effectively increasing the sample size and boosting the sensitivity and specificity of learned interactions. We use the assumption that similar genes have similar regulatory interactions, and share information about putatively conserved interactions. We created synthetic networks to approximate two related biological processes, then evaluated performance of our fused L2 regression, which learns the networks simultaneously given a prior on the relatedness of interactions. We compared recovery of the two artificial 10 TFs by 200 genes networks, using fused L2 versus learning networks separately, and varied the amounts of simulated expression data samples made available to the solver. We measured network inference accuracy with the mean squared error between generated (true) network weights and inferred weights. When the amount of data from the second species was held constant, increasing the amount of data available for learning the network for the first species resulted in a more accurate network prediction, as expected  (figure \ref{fusion}b). When we increased the amount of data from the second species, we obtained performance gains on network one using fused L2 regression, demonstrating our ability to improve network inference on one dataset through incorporation of a related dataset.

When disparate data is generated from identical processes, the weight assigned to fusion constraints should be very large. On the other extreme, when the networks have diverged considerably, a large fusion weight may impair network recovery. We performed an experiment on simulated data in a multi-species network-inference problem to assess the effect of the similarity of networks, and the effect of the fusion weight on network recovery (figure \ref{syntheticgrid}). We created pairs of synthetic networks with varying similarity. The main factors governing similarity between our generated networks was the extent (and accuracy) of the orthology mapping, and the variability between conserved, interactions (in other words, how well conserved the conserved subnetwork was). We conducted a series of simulations to assess the effect of increasing orthology coverage on network recovery.

As expected, when the conserved subgraphs were very similar - when the interactions in the conserved subnetwork were nearly identical, simulating the case of closely related organisms or similar processes (figure \ref{syntheticgrid}a) - increasing the weight of the fusion penalty $\lambda_S$ improved network recovery. As the size of the conserved subgraph increased, this effect was enhanced.

To simulate the case of distantly related organisms, we created networks where the differences between conserved interaction weights were drawn from high variance distribution, mimicking weak conservation (figure \ref{syntheticgrid}b). When the conservation is weak - when the interactions in the conserved subnetwork are nearly as different across species as randomly chosen interactions - performance may not continue to improve by increasing the fusion weight. We showed that even for networks where the conserved subgraph was weakly conserved, there exists a 'sweet spot' where fusion regression improves network recovery. 


\subsection{Fused regression improves performance on both the constrained and non-constrained parts of the network}
Our method learns related networks simultaneously, using predictions about conservation of TF-gene interactions to share information between sources. Our approach is useful for learning networks from similar sources such as related cell types from the same species, where there exists a one-to-one mapping of genes, as well as datasets where the orthology mapping does not span all the genes. This can occur when using different technology, eg microarray and RNAseq, where one method does not assay all of the genes that another one does, but the networks are expected to be very similar; an example of the latter case occurs when inferring networks describing distantly related species, in which case the networks are expected to be more different than similar. When orthology is not complete, we are interested in knowing if performance gains from fused regression are limited to those interactions which have fusion constraints, or if they extend to the entire network. To test this, we used sets of 20 TF by 200 gene synthetic networks, with varying proportions of orthologous TFs and genes. We divided networks into those interactions with fusion constraints (interactions where there exist putative analogous interactions in the second network based on the orthology mapping between TF, gene pairs), which we call the constrained subnetwork, and interactions without fusion constraints, which we refer to as the non-constrained subnetwork. We varied the weight on the fusion penalty, lamS, and evaluated performance by computing AUPR on the constrained subnetwork, the non-constrained subnetwork, and the whole network (figure \ref{fusion}c). Since the conserved subgraphs were similar to each other (more like figure \ref{syntheticgrid}a than \ref{syntheticgrid}b), we expected performance to improve as the fusion penalty weight increased. We observed this, particularly for the constrained subnetwork. As $\lambda_S$ increased, interactions with fusion constraints were encouraged to be more similar. Interestingly, performance gains were seen even in the portion of the network that was unconstrained by fusion. By constraining part of an undetermined system, we obtained gains in even the unconstrained part. 


\subsection{Adaptive fusion successfully identifies and unfuses 'neofunctionalized' genes}
We recognize that orthology prediction is not a perfect proxy for functional conservation \cite{gabaldon_functional_2013, studer_how_2009, nehrt_testing_2011}. We implemented an adaptive fusion algorithm that attempts to optimize a nonconvex saturating penalty function on differences between fused interactions (figure \ref{adapt-schematic}). Pairs of interactions which are very dissimilar even after fusion, which sit in the flat portion of this penalty function, are effectively ``unfused,'' and no further penalty is incurred as differences in interaction weights grow. Our network procedure strongly favors similarity of fused interactions, and only ``unfuses'' interactions when their similarity cannot be reconciled with expression data. Interactions are only ``unfused'' if there is no network configuration consistent with data from both organisms. As a result, the ``unfusing'' or relaxation of the fusion penalty on certain constraints is much more direct evidence for neofunctionalization than comparing separately fit networks could provide. 

We performed a simulation to assess the ability of our adaptive fusion algorithm to learn which parts of two input networks are conserved (figure \ref{adaptivefusion}). We generated synthetic fused networks and introduced error in the fusion constraints by adding false positives and negatives to the orthology information given to the solver. Because we knew which entries in the orthology mapping were ``incorrect'' (not reflected in the generation of the networks), we could correctly label fusion constraints that involved one or more ``incorrect'' mappings. We verified that adaptive-fusion unfused mostly ``incorrectly fused'' interactions (figure \ref{adaptivefusion}a red dots), while leaving truly analogous interactions fused (figure \ref{adaptivefusion}a green dots). Note that while pairs of interactions between true orthologs were sampled randomly so that the difference in interactions tended to be small, and pairs of interactions between incorrect orthologs were sampled randomly so that the difference in interactions tended to be large, there was an element of randomness in the sampling. This resulted in a small fraction of interactions between incorrect orthologs being similar, and similarly, a small fraction of interactions between correct orthologs being different. Consequently, some interactions between incorrect orthologs remained fused and some interactions between correct orthologs were unfused. 

We compared the recovery of interaction weights which were accurately fused, and recovery of interaction weights which were inaccurately fused due to incorrect orthology information (figure \ref{adaptivefusion}b). Because fused L2 heavily penalizes large differences between weights which are predicted to be similar, it is able to retrieve a more accurate network for those interactions with true fusion constraints than by learning networks separately (measured by MSE between the true and inferred interaction weights) . In this simulation, however, the gains accomplished through fused regression do not extend to those interactions lacking true fusion constraints, and the error remains similar to learning networks separately. When we applied adaptive fusion, we did not observe an improvement in network recovery (relative to fused L2). However, we were able to identify fusion constraints reflecting incorrect orthology information that had been provided to the algorithm (figure \ref{adaptivefusion}a). Even in situations where adaptive fusion does not substantially improve recovery - which may occur when unfused interactions remain underconstrained - the approach shows promise in identifying non-conserved interactions, which is of direct interest to systems biology. 


\subsection{Cross-species network inference using bacterial data}
We used gene-expression data from \textit{Bacillus subtilis} and \textit{B. anthracis} in order to assess performance gains of fused regression on real data. Our \textit{B subtilis} data set consists of 360 time-series and steady-state observations of 4891 genes, 4100 of which are protein coding \cite{kunst_complete_1997}, during the life cycle. Our \textit{\textit{B. anthracis}} dataset consists of 72 time-series and steady-state observations of 5536 genes comprising data from distinct points in the life cycle and iron-starvation conditions. There were 247 known transcription factors (TFs) in the \textit{\textit{B. subtilis}} dataset, and 248 TFs in the \textit{B. anthracis} dataset. 

We obtained 1,870 one-to-one orthologs from Inparanoid \cite{ostlund_inparanoid_2010}, 95 of which are transcription factors, which produced 177,650 fusion-constraints between gene interactions within the two species. This number represents only $14.7\%$ of the regulatory interaction matrix in \textit{B. subtilis} and $12.9\%$ in \textit{B. anthracis}. The use of such dissimilar species is a test of whether fusion can improve network-inference even when the overlap in interactions is very small. 

To assess network inference performance, and for use as priors, we used a gold standard of 3,040 known \textit{B. subtilis} interactions with corresponding activation and repression sign. Of these 3,040 priors, 968 had corresponding interactions in \textit{B. anthracis}. Based on our simulation results, we can expect the greatest gains in network-inference performance from fusion when the species of interest has a small number of available conditions, but data is abundant in a related species. However, in order to evaluate performance objectively a gold-standard of known interactions is necessary. As a result, we can only evaluate network recovery for \textit{B. subtilis}, and \textit{B. subtilis} also has the majority of our conditions. In order to simulate the data-poor regime, we subsampled our \textit{B. subtilis} data. Specifically, we divided our \textit{B. subtilis} data into $k$ folds, and then for each fold fit a network to the \textit{B. subtilis} data from that fold alone fused to the entire 72 \textit{B. anthracis} conditions. We can then compute performance metrics (ie AUPR) for each fold, and use their variability across folds to test for significance of any improvements relating to the use of fusion constraints (figure \ref{xspecies-real}a). 

Though overall performance is hindered by our subsampling -- a necessary procedure to allow evaluation of networks, we demonstrate marked improvement in learning the \textit{B. subtilis} network when using fused regression (figure \ref{xspecies-real}a). Notably, these performance gains occur mostly at low values of recall. Essentially, precision increases for the high confidence part of the network but less so for the low confidence part of the network. As a result, the improvements in performance may be more useful than would be suggested by AUPR alone. Interactions suggested by genome wide analysis are useful for target prediction, and the ranking of interactions produced by a network inference algorithms is a useful guide to what order these experiments should be carried out in. The high confidence part of the network represents interactions that are likely candidates for experimental validation, so improvement in recovery of these interactions is particularly useful in guiding future experiments. 

\subsection{Adaptive fusion using bacterial data}
We anticipated that the fusion constraints we introduced based on orthology reflected a mixture of conserved and non-conserved interactions. The goal of adaptive fusion is to unfuse constraints between non-conserved interactions, while leaving intact all other constraints. However, because we lacked a gold standard of known non-conserved interactions between \textit{B. subtilis} and \textit{B. anthracis}, we were unable to directly evaluate how accurately adaptive fusion identified these interactions. We opted instead to introduce a large number of random fusion constraints between genes not known to be orthologous. These fake constraints, which are unlikely to reflect any conserved network structure, served as a proxy for the unknown fraction of non-conserved interactions between orthologous genes. We ran adaptive fusion to learn networks for \textit{B. subtilis} and \texti{B. anthracis}, using these constraints, along with those generated by known orthology. We confirmed that fake fusion constraints were unfused at a higher rate than those generated by known orthologs (figure \ref{xspecies-real}b). Although it may seem odd that a large fraction of fake constraints were left intact, we note that biological networks tend to be sparse, so that many of the random fusion constraints are between coefficients with near zero weight (and therefore near zero difference in weight) (figure \ref{xspecies-real}c). 

\subsection{Integrating datasets using fused regression}
Although there are many large-scale collaborations which attempt to make protocols as uniform as possible for comparability between datasets generated by different labs \cite{paten_nih_2015,kundaje_integrative_2015}, there still exists technical and biologial variability between many experiments attempting to capture the same or similar experimental conditions. With the advent of new technology, such as RNAseq and single-cell RNAseq, microarray is no longer the dominant assay for genome-wide expression, but a large body of accumulated legacy data remains useful, if it can be integrated with more modern techniques. Currently, the most widely used approach to combining datasets for network inference is to learn networks from disparate datasets separately, then rank combine the networks as in Marbach et al \cite{marbach_revealing_2010}. We compared this approach to using fusion regression. We included, along with our \textit{B. subtilis} dataset, a previously published dataset containing 269 samples covering 104 conditions, obtained using a different tiling microarray (vs custom microarray) and different strain of \textit{B. subtilis} \cite{nicolas2012condition}. We compared performance when learning the networks separately, rank combining, and learning the networks simultaneously using fusion regression, and showed large improvement on network inference using our fused L2 approach (figure \ref{fusedl2-real}a). 

\subsection{Within-species fusion using similarity of promoter region}
Information about the similarity of TF-gene interactions can also come from knowledge about the promoter region; this may be a better predictor of target similarity than orthology. In bacteria, genes within the same operon are under the control of the same promoter \cite{lawrence_shared_2002}. We predicted, therefore, that genes within the same operon will be regulated similarly by the same transcription factors. We applied fusion regression by creating fusion constraints between a given transcription factor and genes within the same operon, and showed a boost in \textit{B. subtilis} network recovery using within-species fusion (figure \ref{fusedl2-real}b). 

\subsection{Transcription factor activity estimation integrates into fusion regression approach}
We tested a combination of our fused regression approach with a method for estimating transcription factor activities (TFA). Rather than modeling gene expression using transcription factor mRNA abundance, we fit gene expression as a function of transcription factor activity, as applied to \textit{B. subtilis} by Arrieta-Ortiz et al \cite{arrieta-ortiz_experimentally_2015}. TFA captures transcription factor activities through mechanisms such as dimerization and interaction with required factors, and is a better predictor of TF function than expression level alone \cite{fu_reconstructing_2011}. We estimated TFA based on known regulatory interactions using network component analysis \cite{liao2003network}. To test the integration of this approach with our fused regression, we assessed the combination of \textit{B. subtilis} datasets, as in figure \ref{fusedl2-real}a, with the incorporation of TFA estimation. We randomly divided the prior known interactions in half, and used half to learn TFA. The rest we reserved as a gold standard for validation. As in previous studies, we observed a marked improvement in network inference when using transcription factor activity (figure \ref{tfa}). We also obtained AUPR improvement when using fused regression on TFA, and showed that our gains from sharing information across datasets using fused regression were preserved and even enhanced by using TFA. 

\section{Discussion}
Gene expression data, such as microarray or RNA seq, provide information about the relationship between genes by allowing an experimenter to measure correlations in expression value over time or across conditions. Many sources of information - such as the knowledge that two genes are related through orthology or belong to the same operon - provide information about the relationships between these gene-gene relationships, without necessarily providing information about the gene-gene relationships themselves. For example, the meta information that two genes belong to the same operon suggests that they are likely to have a similar set of regulators \cite{lawrence_shared_2002}, but does little to inform the identity of those regulators. Meta-information about the structure of gene regulatory networks - specifically which pairs of interactions are \textit{a priori} likely to be similar to one another, can provide a powerful set of constraints to improve network inference performance. 

We present a general framework for gene regulatory network inference that incorporates this meta-information - termed fusion constraints - and apply the technique to the problem of simultaneous inference of regulatory networks in multiple species (\textit{B. subtilis} and \textit{B. anthracis}). We develop two algorithms for solving fused network inference problems, each of which has as its core the solving of a least-squares optimization problem with novel regularization constraints.

The first method, fused L2, penalizes the squared difference of the regulatory weights of fused interactions. These constraints improve the performance of the underdetermined network inference problem, by allowing information to be shared across sources. We apply this algorithm to the problem of network inference in two distantly related biological organisms -- \textit{B. subtilis} and \textit{B. anthracis} -- and show that network recovery is improved through the introduction of fusion constraints between pairs of orthologous genes. Existing methods for cross-species network inference operate on the conserved subset of orthologous genes \cite{dillman_comparative_2015}. This may be appropriate with very closely related species, but could not be applied in this domain, where a large fraction (X\% and Y\%) of each species genomes do not have orthologs. Our method, in contrast, can obtain improvements in network inference performance even when the conserved subset of genes is small. We further demonstrate the viability of fused L2 as a method for combining data from multiple experimental platforms, where fusion is between each identical regulator-gene pair. 

Because the algorithm we developed can accomodate constraints between arbitrary pairs of regulatory interactions, any biological prior representing information about expected regulatory similarity can be represented, even if the prior provides no information about the magnitude or direction of regulation. We demonstrate this flexibility through the novel incorporation of operon structure into the gene-regulatory network inference problem. In this application, fusion reflects the assumption that genes in the same operons have similar regulators \cite{lawrence_shared_2002}. The ability to incorporate multiple data sets describing related processes, as well as multiple data types, in a principled manner, helps us take advantage of the breadth of experimentation in biology to better learn the structure of gene regulation. We illustrate this by combining two different \textit{B. subtilis} datasets and show that fused L2 is an improvement over current approaches to combining data \cite{marbach_revealing_2010} because of our ability to exploit the statistical power that our expanded datasets afford us. This approach is particularly interesting in light of the diversity of model organisms used in modern biology. Different model systems provide different advantages and disadvantages for experimental design \cite{stolfi_genetic_2012}, but without a principled mechanism for combining data from multiple sources, it is difficult to fully leverage data obtained from even a slightly different model system. 

Although it is important to take advantage of the similarities of related organisms for generating improved models of gene regulation, it is also critically important to understand how systems differ from one another. Our cross-species network inference method is premised on the assumption that orthologous genes have similar regulators. Existing approaches to the genome-wide testing of this assumption learn regulatory networks separately, then compare to identify conservation \cite{aytes_cross-species_2014, Wang2014a}. But because network inference is typically underconstrained, fitting a network that describes a particular set of experimental observations involves sampling a single network from a large set of networks that fit the data equally (or almost equally) as well. As a result, the existence of a difference between corresponding regulatory interactions in a pair of experimentally derived networks is weak evidence that a difference truly does exist. It may be that some other pair of networks exist that fit the data nearly as well, but do not exhibit this difference. As a result, global network inference algorithms are a very weak tool for uncovering evolutionary divergence. Our method, on the other hand, explicitly favors recovering networks for which evolutionarily corresponding interactions are similar. As a result, the failure to obtain networks that confirm evolutionary conservation is strong evidence that conservation does not exist; the next best network that does exhibit conservation must fit the data much worse to have overcome the bias built into the fusion constraints. 

Because our approach allows us to directly test -- and reject -- the hypothesis of conserved function, and because the enforcement of fusion constraints between non-conserved interactions may degrade network inference performance, we introduce an adaptive fusion method that decouples pairs of interactions for which expression data suggests a lack of conservation. Like the fused L2 approach, this method attempts to solve for networks which minimize a function that minimizes error in fitting the expression data plus error associated with the fusion penalty. In the case of cross-species network inference, orthology may provide a useful set of constraints on network structure. However, orthology does not always accurately predict functional similarity \cite{gabaldon_functional_2013}.When a fusion constraint encourages orthologous gene, TF pairs to share similar interactions across species, but the expression data suggests that these interaction weights should not be similar, the algorithm will arrive at some compromise between these two forces. If the disagreement arose because the genes involved have taken on new function through evolutionary divergence \cite{kellis_proof_2004}, network recovery would be improved had the fusion constraint not been enforced. We introduce a method -- adaptive fusion -- that attempts to learn which fusion constraints should be relaxed while the network is being learned. This method is based on minimizing a saturating penalty function on fusion constraints, similar to a class of penalties that have been developed to minimise bias in regularized regression \cite{fan2001variable,zhang2010nearly}. The result of adaptive fusion is both a network and a new set of fusion constraints, describing the learned conservation weights (including which fusion constraints have been relaxed). For the multiple species case, relaxation of fusion constraints represents orthologs which do not share similar interactions. When jointly learning networks describing processes in different cell lines, this may identify interesting context-specific behavior. Genes may be fused together on the basis of similar binding sites or chromatin features, and the relaxing of the fusion penalty indicates divergence of gene function. Adaptive fusion, therefore, is a tool for network inference as well as a method for testing network conservation. 

Because our model shares its basic assumptions about the role of transcription factors in gene expression dynamics with models developed for single-species network inference, we are able to leverage techniques developed for the single-species estimation of transctiption factor activity \cite{fu_reconstructing_2011}. The performance gains of this additional step in the cross-species case are significant.

Our approaches -- fused L2 and adaptive fusion -- represent a very general framework for simultaneous network inference and the incorporation of structured biological priors. These priors -- incorporated into our method as fusion constraints -- allow the use of rich sources of biological knowledge, such as orthology and operon structure, which have informed experimental design, but are typically not incorporated into genome wide network inference algorithms. By accomodating the simultaneous inference of multiple related networks, we can improve network inference performance by allowing the efficient reuse of data from similar, but not necessarily identical, sources. A method for pooling data from multiple sources holds the promise of vastly expanding the quantity of data available for analysis, particularly in less commonly used model systems. At the same time these methods allow us to test our assumptions on how similar biological systems relate to one another, by allowing us to rule out conservation in a principled way, and at the genome-wide scale. 


%One future aim is the ability to pick the $a$ parameter controlling the saturation point of the adaptive fusion penalty automatically from the expression data; this parameter is biologically interesting and indicates the difference in interaction weights which is great enough that the fusion constraints should be relaxed. An exhaustive search through different combinations of putative fusion constraints is prohibitive, but perhaps there is another way which avoids this. 


%\nocite{*}
\bibliographystyle{plain}
\bibliography{paper1.bib}


\begin{figure}
\begin{center}
  \includegraphics[scale=0.45]{fig1.pdf}
  \caption{\label{fusion} a. schematic representation of gene relationships b. We generate two networks with 10 TFs by 200 genes, with 75\% sparsity, 0.1 measurement noise, and 0.3 fusion noise. The x-axis represents the number of conditions drawn from species 1, and the y-axis tracks the number of conditions from species 2 used. Performance is mean square error of network 1; as the number of conditions from the first species increases, our ability to recover network 1 also improves. But if we increase the number of conditions from the related species 2, we are better able to learn the network describing species 1. c. We generate a series of networks with 20 TFs by 200 genes, with 50\% sparsity, 0.1 measurement noise, and 0.1 fusion noise. We increase amount of genes fused from 0 to 0.75, with 0.25 increments and evaluate performance on the whole network, interactions which have fusion constraints (constrained part of the network), and interactions which do not have fusion constraints (non-constrained), and the effect of increasing the fusion weight.}
\end{center}
\end{figure}


\begin{figure}
\begin{center}
  \includegraphics[scale=0.45]{fig2.pdf}
  \caption{\label{syntheticgrid} a. We generate a series of networks with 10 TFs by 200 genes, with 75\% sparsity, 0.1 measurement noise, and 0.1 fusion noise. We hold lamR constant at 1.0 and 2.0 respectively. The x-axis represents lamS and the y-axis represents the proportion of nodes which are orthologous. As the networks increase in similarity, performance improves and benefits from a large fusion penalty weight. We measure performance by showing ratio of mean squared error relative to MSE without fusion. b. Here is another set of networks of 10 TFs by 200 genes, with 75\% sparsity 0.1 measurement noise, and 0.75 fusion noise. We hold and lamR constant at 1.0 and 2.0 respectively and measure performance in the same way as previous figure.}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
  \includegraphics[scale=0.45]{fig3.pdf}
  \caption{\label{adapt-schematic} a. Adaptive fusion is a quadratic around the origin, and tapers off and plateaus, allowing differences in fused interaction weights to grow if the expression data supports this, without incurring further penalty from the fusion. b. The adaptive fusion penalty is modified from SCAD (smoothly clipped absolute deviation) and MCP (minimax concave penalty) functions. It shares with these penalties the zero derivative further from the origin. }
\end{center}
\end{figure}

\begin{figure}
\begin{center}
  \includegraphics[scale=0.45]{fig4_2.png}
  \caption{\label{adaptivefusion} a.  Here we use two network of 35 TFs by 200 genes, with 0.4 of the genes and TFs fused, 0.1 measurement noise, and 0.1 fusion noise. We hold lamP and lamR constant at 1, and solve without fusion, using L2 fusion, and adaptive fusion, with lamS = 10 for both L2 and adaptive fusion. We plot the interaction weights; each point's coordinates represents the weight for network 1 and the weight for network 2. The solver was given a list of orthologs; the green points represent interactions between TFs and genes whose orthology information is correct, and the red points represent interactions between TFs and genes, where the orthology of one or both is incorrectly reported to the solver. We see that the L2 forces the points onto a diagonal, and that fused interactions adopt similar weights, regardless of the accuracy of the fusion. Our adaptive fusion solver is able to correctly identify and 'unfuse' incorrectly fused interactions. b. Here we plot the mean squared error of correctly fused and incorrectly fused (green and red dots, respectively) interactions for each solver.}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
  \includegraphics[scale=0.45]{fig5_2.pdf}
  \caption{\label{xspecies-real} a. Using optimal lamR and lamP values, we use 10-fold cross validation to learn B. subtilis network. We compare performance when solving without fusion, and using L2 fusion with lamS = 1.0. Mean AUPR for lamS = 0 (unfused): 0.0619. Mean AUPR for lamS = 1: 0.0839. b. We test adaptive fusion using the same setup, with the addition of false orthology information. We set a 30\% false positive rate, including 561 additional orthologs. We run adaptive fusion, setting the $a$ term equal to the value above which 40\% of constraints would unfuse, reflecting our belief that in addition to some of the known orthologs result in fusion constraints which should be relaxed. Here we show the percentage of constraints relaxed, from constraints created from the false orthologs and the known orthologs. We also show the distributions of differences in weights corresponding to fused constraints created from the false orthologs and the known orthologs, when networks are solved separately. }
\end{center}
\end{figure}


\begin{figure}
\begin{center}
  \includegraphics[scale=0.45]{fig6.pdf}
  \caption{\label{fusedl2-real} a. We use optimal lamR and lamP values, and solve for B. subtilis-1 and B. subtilis-2 networks using 10-fold cross validation. We fuse our original B. subtilis-1 dataset to another B. subtilis dataset, which we call B. subtilis-2, using orthology information, and evaluate performance of B. subtilis-1 using AUPR on gold standard. We compare L2 fusion with solving the networks separately without fusion, then rank combining as in Marbach et al., as well as solving B. subtilis on its own. b. We again use optimal lamR and lamP values, and solve for the B. subtilis network using 10-fold cross validation, and evaluate using AUPR on B. subtilis with gold standard. Here, we fuse together TF, gene pairs, fixing the same TF with genes in the same operon group. The assumption is that genes in the same operon should be regulated similarly by the same TFs. We compare L2 fusion performance using operons with unfused.}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
  \includegraphics[scale=0.45]{fig7.pdf}
  \caption{\label{tfa} We integrate transcription factor activity (TFA) into our network inference, and show that fusion improves performance.Here we learn the B. subtilis-1 network with and without fusion, and with and without TFA}
\end{center}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=.5,trim=0mm 0mm 0mm 0mm,clip]{party3.eps}
    \caption{\label{cartoon} Schematic representation of design matrix construction. Here, the circles and hexagons correspond to different species. Bidirectional arrows represent orthology information and dotted arrows represent putative interactions between TFs and genes. Rectangles represent matrices; because weights can be solved independently unless there exists fusion constraints between them, we identify related weights and construct matrix for solving. }
\end{figure}

\end{document}


